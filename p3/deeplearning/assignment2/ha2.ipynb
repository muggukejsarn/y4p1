{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 Deep Learning - CNNs\n",
    "\n",
    "_Magnus Caesar_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from load_func import load_mnist_func\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Multi-layer fully connected neural network\n",
    "Implement exactly the same network as for assignment 1.\n",
    "\n",
    "1. Compare the performance\n",
    "2. Compare speed\n",
    "3. Learning curve plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first network from assignment 1 is a linear model. The input dimension is 784 (28x28 pixels) and the output dimension is 10. The second network is multilayered. The one I implemented had 2 hidden layers á 100 neurons each, which the same necessary input and output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poopdogs\n",
      "Reading MNIST: stationary\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "X_train, Y_train, X_test, Y_test = load_mnist_func(\"station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data into pytorch's domain\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDataset.\n",
    "train_dataset = data.TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "test_dataset = data.TensorDataset(X_test_tensor, Y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base network and forward**\n",
    "\n",
    "Basic class for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_hidden_layers, input_dim=784, output_dim=10, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList([]) # Ska ha lista med lager / moduler\n",
    "\n",
    "        # If no hidden layers\n",
    "        if num_hidden_layers == 0: \n",
    "            self.layers.append(torch.nn.Linear(input_dim, output_dim, dtype=torch.float32))\n",
    "\n",
    "        # If hidden layers layers\n",
    "        # TODO\n",
    "        # this might have some logic flaws\n",
    "        else:\n",
    "            # First layer\n",
    "            self.layers.append(torch.nn.Linear(input_dim, hidden_dim))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            \n",
    "            # Add extra layers\n",
    "            for _ in range(num_hidden_layers):\n",
    "                self.layers.append(torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "                self.layers.append(torch.nn.ReLU())\n",
    "            \n",
    "            self.layers.append(torch.nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing loss**\n",
    "\n",
    "This is the exact same \"compute_loss\" function used in lab 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, batch):\n",
    "    # forward pass and loss function\n",
    "\n",
    "    inp_data, labels = batch # Read current batch\n",
    "\n",
    "    output = model(inp_data) # Get output from model\n",
    "    loss = torch.nn.functional.cross_entropy(output, labels)\n",
    "    acc = (output.argmax(dim=1) == labels.argmax(dim=1)).float().mean() # Check accuracy\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizing**\n",
    "\n",
    "The optimizer is SGD (as opposed to ADAM which comes later). PyTorch supports momentum, but that wasn't used in lab 1 and so won't be used here either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sympy' has no attribute 'printing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SeqNetwork(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;66;03m# size used in lab1\u001b[39;00m\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\optim\\sgd.py:63\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable, fused)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov \u001b[38;5;129;01mand\u001b[39;00m (momentum \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dampening \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_supports_amp_scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\optim\\optimizer.py:377\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    374\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_compile.py:27\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, exc, logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging, trace_rules, variables\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\exc.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompileId\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\utils.py:66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytree\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fx\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ordered_set\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedSet\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     75\u001b[0m     Application,\n\u001b[0;32m     76\u001b[0m     CeilToInt,\n\u001b[0;32m     77\u001b[0m     CleanDiv,\n\u001b[0;32m     78\u001b[0m     FloorDiv,\n\u001b[0;32m     79\u001b[0m     FloorToInt,\n\u001b[0;32m     80\u001b[0m     IsNonOverlappingAndDenseIndicator,\n\u001b[0;32m     81\u001b[0m     Mod,\n\u001b[0;32m     82\u001b[0m     PythonMod,\n\u001b[0;32m     83\u001b[0m )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m int_oo\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprinters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PythonPrinter\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\utils\\_sympy\\functions.py:185\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gcd  \u001b[38;5;66;03m# type: ignore[return-value]  # remove in py3.12\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# It would be nice to have assertions on whether or not inputs is_integer\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# However, with bugs like https://github.com/sympy/sympy/issues/26620 sympy\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# sometimes inconsistently reports floats an integers.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Right now, FloorDiv de facto changes behavior if arguments are negative or\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# not, this can potentially cause correctness issues.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mFloorDiv\u001b[39;00m(sympy\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[0;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    We maintain this so that:\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    1. We can use divisibility guards to simplify FloorDiv(a, b) to a / b.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    NB: This is Python-style floor division, round to -Inf\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     nargs: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m,)\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\utils\\_sympy\\functions.py:206\u001b[0m, in \u001b[0;36mFloorDiv\u001b[1;34m()\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdivisor\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m sympy\u001b[38;5;241m.\u001b[39mBasic:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sympystr\u001b[39m(\u001b[38;5;28mself\u001b[39m, printer: \u001b[43msympy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprinting\u001b[49m\u001b[38;5;241m.\u001b[39mStrPrinter) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    207\u001b[0m     base \u001b[38;5;241m=\u001b[39m printer\u001b[38;5;241m.\u001b[39mparenthesize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase, PRECEDENCE[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtom\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    208\u001b[0m     divisor \u001b[38;5;241m=\u001b[39m printer\u001b[38;5;241m.\u001b[39mparenthesize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdivisor, PRECEDENCE[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtom\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sympy' has no attribute 'printing'"
     ]
    }
   ],
   "source": [
    "model = SeqNetwork(0)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0)\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 100 # size used in lab1\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "# Utilize mini-batch gradient descent\n",
    "# After each batch: collect training cost and accuracy\n",
    "# After each epoch (all batches: full data set)\n",
    "# Epochs training accuracy is the average\n",
    "# After each epoch: calculate test cost and accuracy\n",
    "\n",
    "# These four lists are requested in the assignment\n",
    "# Every batch\n",
    "train_accuracies = []\n",
    "train_costs = []\n",
    "\n",
    "# Every epoch\n",
    "test_costs = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Same training structure as from lab 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(f\"Entering epoch {epoch+1} out of {num_epochs}.\")\n",
    "\n",
    "    for b in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss, train_acc = compute_loss(model, b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Appending training metrics\n",
    "        train_costs.append(loss.item())\n",
    "        train_accuracies.append(train_acc)\n",
    "    \n",
    "    # Appending test metrics\n",
    "    temp_costs = []\n",
    "    temp_acc = []\n",
    "    for b in test_loader:\n",
    "        loss, acc = compute_loss(model, b)\n",
    "        temp_costs.append(loss.item())\n",
    "        temp_acc.append(acc)\n",
    "    \n",
    "    test_costs.append(np.mean(temp_costs))\n",
    "    test_accuracies.append(np.mean(temp_acc))\n",
    "\n",
    "    print(f\"Current testing loss: {test_costs[-1]}\")\n",
    "    print(f\"Current test accuracy: {test_accuracies[-1]}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Producing plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Multilayer convolutional neural network\n",
    "CNN using PyTorch with SGD and cross-entropy loss. Reach 98% accuracy.\n",
    "\n",
    "1. How many learnable weights does the network contain? Compare with previous exercise.\n",
    "2. Learning curve plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base class CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new datasets with dimensions tailored for CNN networks\n",
    "train_dataset_cnn = data.TensorDataset(X_train_tensor.view(60000, 1, 28, 28), Y_train_tensor)\n",
    "test_dataset_cnn = data.TensorDataset(X_test_tensor.view(10000, 1, 28, 28), Y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetwork(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.pool = torch.nn.MaxPool2d(2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        #relu\n",
    "        #pool\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        #relu\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fullyconnected = torch.nn.Linear(32*7*7, 10)\n",
    "        #self.softm = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        self.components = torch.nn.ModuleList([self.conv1,\n",
    "                                               self.relu,\n",
    "                                               self.pool,\n",
    "                                               self.conv2,\n",
    "                                               self.relu,\n",
    "                                               self.pool,\n",
    "                                               self.conv3,\n",
    "                                               self.relu,\n",
    "                                               self.flatten,\n",
    "                                               self.fullyconnected])\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        for l in self.components:\n",
    "            x = l(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizing and training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, traindata, testdata, num_epochs=40, batch_size=100, optmzr=\"sgd\"):\n",
    "    \"\"\"\n",
    "    Train model.\n",
    "    Input\n",
    "        model:              a subclass of torch.nn.Module to be trained\n",
    "        traindata:          self-explanatory. Must be of type torch.TensorDataset\n",
    "        testdata:           see above\n",
    "        num_epochs:         # epochs trained [int]\n",
    "        batch_size:         # batches [int]\n",
    "        optmzr:             optimizer. Select 'sgd' or 'adam' [string]\n",
    "    Returns\n",
    "        x_axis_train:       torch.arange(correct size)\n",
    "        x_axis_test:        torch.arange(test size)\n",
    "        train_costs:        [size 60000 / batches * num_epochs]\n",
    "        train_accuracies:   [size 60000 / batches * num_epochs]\n",
    "        test_costs:         [size num_epochs]. Average test cost every epoch\n",
    "        test_accuracies:    [size num_epochs]. Also average across epoch\n",
    "        total_time:         Time spent training the network\n",
    "        avgspeed:           Average time / epoch\n",
    "    Ex. \n",
    "    x_tr, x_te, trc, tra, tec, tac = train(model, train_dataset_cnn, test_dataset_cnn)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Optimizer\n",
    "    if optmzr == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0)\n",
    "    \n",
    "    if optmzr == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters()) # Standard parameters in Adam\n",
    "\n",
    "    # Load data\n",
    "    train_loader = data.DataLoader(traindata, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = data.DataLoader(testdata, batch_size=batch_size)\n",
    "    \n",
    "    # Lists for later plotting\n",
    "    train_costs = []\n",
    "    train_accuracies = []\n",
    "    test_costs = []\n",
    "    test_accuracies = []\n",
    "\n",
    "\n",
    "    # For time tracking\n",
    "    starttime = time.time()\n",
    "\n",
    "    # Same training structure as from lab 2\n",
    "    for epoch in range(0, num_epochs):\n",
    "        model.train()\n",
    "        print(f\"Entering epoch {epoch+1} out of {num_epochs}.\")\n",
    "\n",
    "        for b in train_loader:\n",
    "            # b[0][0][0] är en bild. Ser bra ut: kan plottas\n",
    "\n",
    "            # print(b[0].size())\n",
    "            # plt.imshow(b[0][4][0])\n",
    "\n",
    "            # Training\n",
    "            optimizer.zero_grad()\n",
    "            loss, train_acc = compute_loss(model, b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Appending training metrics\n",
    "            train_costs.append(loss.item())\n",
    "            train_accuracies.append(train_acc)\n",
    "            \n",
    "        # Appending test metrics\n",
    "        temp_costs = []\n",
    "        temp_acc = []\n",
    "        \n",
    "        for b in test_loader:\n",
    "            loss, acc = compute_loss(model, b)\n",
    "            temp_costs.append(loss.item())\n",
    "            temp_acc.append(acc)\n",
    "        \n",
    "        test_costs.append(np.mean(temp_costs))\n",
    "        test_accuracies.append(np.mean(temp_acc))\n",
    "\n",
    "        # Time related\n",
    "        t0 = time.time()\n",
    "        t_tot = t0-starttime\n",
    "        avgspeed = t_tot / (epoch+1)\n",
    "        est_time_remaining = (avgspeed) * (num_epochs - epoch+1)\n",
    "\n",
    "        # Statistics\n",
    "        print(f\"Current testing loss:      {test_costs[-1]:>10.4f}\")\n",
    "        print(f\"Current test accuracy:     {test_accuracies[-1]:>10.2f}\")\n",
    "        print(f\"Ellapsed time:             {round(t0 - starttime, 2):>10.2f} seconds.\")\n",
    "        print(f\"Estimated time remaining:  {round(est_time_remaining, 2):>10.2f} seconds.\")\n",
    "        print()\n",
    "    \n",
    "    tend = time.time()\n",
    "    total_time = t_end-starttime\n",
    "    print(f\"Final results:\")\n",
    "    print(f\"Total time elapsed: {round(total_time, 3)} s\")\n",
    "    print(f\"Average speed: {round(avgspeed, 3)} s / epoch\")\n",
    "\n",
    "    # x-axis for plotting\n",
    "    x_axis_train = torch.arange(len(train_costs)) # For every actual iteration\n",
    "    x_axis_test = torch.arange(len(test_costs)) # Average for every epoch\n",
    "\n",
    "    return x_axis_train, x_axis_test, train_costs, train_accuracies, test_costs, test_accuracies, total_time, avgspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sympy' has no attribute 'printing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;66;03m# size used in lab1\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m trc, tra, tec, tac \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, traindata, testdata, num_epochs, batch_size, optmzr)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optmzr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optmzr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     30\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;66;03m# Standard parameters in Adam\u001b[39;00m\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\optim\\sgd.py:63\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable, fused)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov \u001b[38;5;129;01mand\u001b[39;00m (momentum \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dampening \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_supports_amp_scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\optim\\optimizer.py:377\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    374\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_compile.py:27\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, exc, logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging, trace_rules, variables\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\exc.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompileId\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\utils.py:66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytree\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fx\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ordered_set\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedSet\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     75\u001b[0m     Application,\n\u001b[0;32m     76\u001b[0m     CeilToInt,\n\u001b[0;32m     77\u001b[0m     CleanDiv,\n\u001b[0;32m     78\u001b[0m     FloorDiv,\n\u001b[0;32m     79\u001b[0m     FloorToInt,\n\u001b[0;32m     80\u001b[0m     IsNonOverlappingAndDenseIndicator,\n\u001b[0;32m     81\u001b[0m     Mod,\n\u001b[0;32m     82\u001b[0m     PythonMod,\n\u001b[0;32m     83\u001b[0m )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m int_oo\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprinters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PythonPrinter\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\utils\\_sympy\\functions.py:185\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gcd  \u001b[38;5;66;03m# type: ignore[return-value]  # remove in py3.12\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# It would be nice to have assertions on whether or not inputs is_integer\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# However, with bugs like https://github.com/sympy/sympy/issues/26620 sympy\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# sometimes inconsistently reports floats an integers.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Right now, FloorDiv de facto changes behavior if arguments are negative or\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# not, this can potentially cause correctness issues.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mFloorDiv\u001b[39;00m(sympy\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[0;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    We maintain this so that:\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    1. We can use divisibility guards to simplify FloorDiv(a, b) to a / b.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    NB: This is Python-style floor division, round to -Inf\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     nargs: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m,)\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\utils\\_sympy\\functions.py:206\u001b[0m, in \u001b[0;36mFloorDiv\u001b[1;34m()\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdivisor\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m sympy\u001b[38;5;241m.\u001b[39mBasic:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sympystr\u001b[39m(\u001b[38;5;28mself\u001b[39m, printer: \u001b[43msympy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprinting\u001b[49m\u001b[38;5;241m.\u001b[39mStrPrinter) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    207\u001b[0m     base \u001b[38;5;241m=\u001b[39m printer\u001b[38;5;241m.\u001b[39mparenthesize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase, PRECEDENCE[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtom\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    208\u001b[0m     divisor \u001b[38;5;241m=\u001b[39m printer\u001b[38;5;241m.\u001b[39mparenthesize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdivisor, PRECEDENCE[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtom\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sympy' has no attribute 'printing'"
     ]
    }
   ],
   "source": [
    "model = ConvNetwork()\n",
    "\n",
    "num_epochs = 30\n",
    "batch_size = 64 # size used in lab1\n",
    "\n",
    "trc, tra, tec, tac = train(model, train_dataset_cnn, test_dataset_cnn, num_epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: regular learning curve plot of the CNN: see first page of HA2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: swapping the order of max pooling and the activation function\n",
    "\n",
    "1. How does this affect the models performance? Final accuracy?\n",
    "2. Swapping ReLU to tanh: differences? Time taken, final accuarcy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwapedConvNetwork(torch.nn.Module):\n",
    "    # Easier to create a new class than to fiddle in the old one\n",
    "    def __init__(self, activator: str):\n",
    "        super().__init__()\n",
    "\n",
    "        # Reused layers\n",
    "        if activator.lower() == \"relu\":\n",
    "            self.activator = torch.nn.ReLU()\n",
    "        if activator.lower() == \"tanh\":\n",
    "            self.activator == torch.nn.Tanh()\n",
    "        \n",
    "        self.pool = torch.nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        # Full laid out structure\n",
    "        self.conv1 = torch.nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
    "        # Pool\n",
    "        # Activator\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        # Pool\n",
    "        # Activator\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # Activator\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fullyconnected = torch.nn.Linear(32*7*7, 10)\n",
    "        #self.softm = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        self.components = torch.nn.ModuleList([self.conv1,\n",
    "                                               self.pool,\n",
    "                                               self.activator,\n",
    "                                               self.conv2,\n",
    "                                               self.pool,\n",
    "                                               self.activator,\n",
    "                                               self.conv3,\n",
    "                                               self.activator,\n",
    "                                               self.flatten,\n",
    "                                               self.fullyconnected])\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        for l in self.components:\n",
    "            x = l(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering epoch 1 out of 40.\n",
      "Current testing loss:          2.2707\n",
      "Current test accuracy:           0.26\n",
      "Ellapsed time:                  14.47 seconds.\n",
      "Estimated time remaining:      593.40 seconds.\n",
      "\n",
      "Entering epoch 2 out of 40.\n",
      "Current testing loss:          0.5702\n",
      "Current test accuracy:           0.81\n",
      "Ellapsed time:                  28.42 seconds.\n",
      "Estimated time remaining:      568.48 seconds.\n",
      "\n",
      "Entering epoch 3 out of 40.\n",
      "Current testing loss:          0.3917\n",
      "Current test accuracy:           0.87\n",
      "Ellapsed time:                  42.62 seconds.\n",
      "Estimated time remaining:      554.06 seconds.\n",
      "\n",
      "Entering epoch 4 out of 40.\n",
      "Current testing loss:          0.3504\n",
      "Current test accuracy:           0.88\n",
      "Ellapsed time:                  57.47 seconds.\n",
      "Estimated time remaining:      545.99 seconds.\n",
      "\n",
      "Entering epoch 5 out of 40.\n",
      "Current testing loss:          0.2375\n",
      "Current test accuracy:           0.93\n",
      "Ellapsed time:                  72.20 seconds.\n",
      "Estimated time remaining:      534.29 seconds.\n",
      "\n",
      "Entering epoch 6 out of 40.\n",
      "Current testing loss:          0.1981\n",
      "Current test accuracy:           0.94\n",
      "Ellapsed time:                  86.91 seconds.\n",
      "Estimated time remaining:      521.49 seconds.\n",
      "\n",
      "Entering epoch 7 out of 40.\n",
      "Current testing loss:          0.1680\n",
      "Current test accuracy:           0.95\n",
      "Ellapsed time:                 101.73 seconds.\n",
      "Estimated time remaining:      508.65 seconds.\n",
      "\n",
      "Entering epoch 8 out of 40.\n",
      "Current testing loss:          0.1558\n",
      "Current test accuracy:           0.95\n",
      "Ellapsed time:                 116.49 seconds.\n",
      "Estimated time remaining:      495.07 seconds.\n",
      "\n",
      "Entering epoch 9 out of 40.\n",
      "Current testing loss:          0.1411\n",
      "Current test accuracy:           0.96\n",
      "Ellapsed time:                 131.18 seconds.\n",
      "Estimated time remaining:      481.00 seconds.\n",
      "\n",
      "Entering epoch 10 out of 40.\n",
      "Current testing loss:          0.1241\n",
      "Current test accuracy:           0.96\n",
      "Ellapsed time:                 145.70 seconds.\n",
      "Estimated time remaining:      466.23 seconds.\n",
      "\n",
      "Entering epoch 11 out of 40.\n",
      "Current testing loss:          0.1090\n",
      "Current test accuracy:           0.96\n",
      "Ellapsed time:                 161.19 seconds.\n",
      "Estimated time remaining:      454.28 seconds.\n",
      "\n",
      "Entering epoch 12 out of 40.\n",
      "Current testing loss:          0.1257\n",
      "Current test accuracy:           0.96\n",
      "Ellapsed time:                 176.50 seconds.\n",
      "Estimated time remaining:      441.24 seconds.\n",
      "\n",
      "Entering epoch 13 out of 40.\n",
      "Current testing loss:          0.0978\n",
      "Current test accuracy:           0.97\n",
      "Ellapsed time:                 191.40 seconds.\n",
      "Estimated time remaining:      426.97 seconds.\n",
      "\n",
      "Entering epoch 14 out of 40.\n",
      "Current testing loss:          0.0868\n",
      "Current test accuracy:           0.97\n",
      "Ellapsed time:                 206.29 seconds.\n",
      "Estimated time remaining:      412.57 seconds.\n",
      "\n",
      "Entering epoch 15 out of 40.\n",
      "Current testing loss:          0.0946\n",
      "Current test accuracy:           0.97\n",
      "Ellapsed time:                 221.27 seconds.\n",
      "Estimated time remaining:      398.28 seconds.\n",
      "\n",
      "Entering epoch 16 out of 40.\n",
      "Current testing loss:          0.0816\n",
      "Current test accuracy:           0.97\n",
      "Ellapsed time:                 236.33 seconds.\n",
      "Estimated time remaining:      384.04 seconds.\n",
      "\n",
      "Entering epoch 17 out of 40.\n",
      "Current testing loss:          0.0782\n",
      "Current test accuracy:           0.97\n",
      "Ellapsed time:                 251.15 seconds.\n",
      "Estimated time remaining:      369.33 seconds.\n",
      "\n",
      "Entering epoch 18 out of 40.\n",
      "Current testing loss:          0.0810\n",
      "Current test accuracy:           0.97\n",
      "Ellapsed time:                 266.14 seconds.\n",
      "Estimated time remaining:      354.86 seconds.\n",
      "\n",
      "Entering epoch 19 out of 40.\n",
      "Current testing loss:          0.0743\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 280.92 seconds.\n",
      "Estimated time remaining:      340.06 seconds.\n",
      "\n",
      "Entering epoch 20 out of 40.\n",
      "Current testing loss:          0.0703\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 295.68 seconds.\n",
      "Estimated time remaining:      325.25 seconds.\n",
      "\n",
      "Entering epoch 21 out of 40.\n",
      "Current testing loss:          0.0664\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 310.38 seconds.\n",
      "Estimated time remaining:      310.38 seconds.\n",
      "\n",
      "Entering epoch 22 out of 40.\n",
      "Current testing loss:          0.0637\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 325.62 seconds.\n",
      "Estimated time remaining:      296.02 seconds.\n",
      "\n",
      "Entering epoch 23 out of 40.\n",
      "Current testing loss:          0.0626\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 340.40 seconds.\n",
      "Estimated time remaining:      281.20 seconds.\n",
      "\n",
      "Entering epoch 24 out of 40.\n",
      "Current testing loss:          0.0702\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 355.13 seconds.\n",
      "Estimated time remaining:      266.35 seconds.\n",
      "\n",
      "Entering epoch 25 out of 40.\n",
      "Current testing loss:          0.0619\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 370.08 seconds.\n",
      "Estimated time remaining:      251.66 seconds.\n",
      "\n",
      "Entering epoch 26 out of 40.\n",
      "Current testing loss:          0.0605\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 385.05 seconds.\n",
      "Estimated time remaining:      236.95 seconds.\n",
      "\n",
      "Entering epoch 27 out of 40.\n",
      "Current testing loss:          0.0608\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 399.91 seconds.\n",
      "Estimated time remaining:      222.17 seconds.\n",
      "\n",
      "Entering epoch 28 out of 40.\n",
      "Current testing loss:          0.0555\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 414.90 seconds.\n",
      "Estimated time remaining:      207.45 seconds.\n",
      "\n",
      "Entering epoch 29 out of 40.\n",
      "Current testing loss:          0.0518\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 429.77 seconds.\n",
      "Estimated time remaining:      192.66 seconds.\n",
      "\n",
      "Entering epoch 30 out of 40.\n",
      "Current testing loss:          0.0535\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 444.79 seconds.\n",
      "Estimated time remaining:      177.92 seconds.\n",
      "\n",
      "Entering epoch 31 out of 40.\n",
      "Current testing loss:          0.0592\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 459.62 seconds.\n",
      "Estimated time remaining:      163.09 seconds.\n",
      "\n",
      "Entering epoch 32 out of 40.\n",
      "Current testing loss:          0.0592\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 476.09 seconds.\n",
      "Estimated time remaining:      148.78 seconds.\n",
      "\n",
      "Entering epoch 33 out of 40.\n",
      "Current testing loss:          0.0505\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 490.95 seconds.\n",
      "Estimated time remaining:      133.90 seconds.\n",
      "\n",
      "Entering epoch 34 out of 40.\n",
      "Current testing loss:          0.0479\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 505.72 seconds.\n",
      "Estimated time remaining:      118.99 seconds.\n",
      "\n",
      "Entering epoch 35 out of 40.\n",
      "Current testing loss:          0.0495\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 521.50 seconds.\n",
      "Estimated time remaining:      104.30 seconds.\n",
      "\n",
      "Entering epoch 36 out of 40.\n",
      "Current testing loss:          0.0513\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 536.31 seconds.\n",
      "Estimated time remaining:       89.39 seconds.\n",
      "\n",
      "Entering epoch 37 out of 40.\n",
      "Current testing loss:          0.0481\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 551.14 seconds.\n",
      "Estimated time remaining:       74.48 seconds.\n",
      "\n",
      "Entering epoch 38 out of 40.\n",
      "Current testing loss:          0.0474\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 566.15 seconds.\n",
      "Estimated time remaining:       59.59 seconds.\n",
      "\n",
      "Entering epoch 39 out of 40.\n",
      "Current testing loss:          0.0451\n",
      "Current test accuracy:           0.99\n",
      "Ellapsed time:                 580.89 seconds.\n",
      "Estimated time remaining:       44.68 seconds.\n",
      "\n",
      "Entering epoch 40 out of 40.\n",
      "Current testing loss:          0.0511\n",
      "Current test accuracy:           0.98\n",
      "Ellapsed time:                 595.75 seconds.\n",
      "Estimated time remaining:       29.79 seconds.\n",
      "\n",
      "Final results:\n",
      "Total time elapsed: 595.7467067241669 s\n",
      "Average speed: 14.893667668104172 s / epoch\n"
     ]
    }
   ],
   "source": [
    "model = SwapedConvNetwork(\"relu\")\n",
    "\n",
    "x_train, x_test, trc2, tra2, tec2, tac2, time, avgspeed = train(model, train_dataset_cnn, test_dataset_cnn, batch_size=64, optmzr=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with ex2:\n",
    "# 3a\n",
    "# Longer or faster training time?\n",
    "# Final accuracy?\n",
    "# Learning curve plot: both models next to each other!\n",
    "\n",
    "# 3b: ReLU -> tanh\n",
    "# longer or faster training time?\n",
    "# Final accuracy?\n",
    "# learning curve plot !\n",
    "\n",
    "# 3c conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: swapping SGD for ADAM\n",
    "\n",
    "Are good results obtained faster? Compare with SGD; provide learning curve plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ex4_sgd = SwapedConvNetwork(\"relu\")\n",
    "model_ex4_adam = SwapedConvNetwork(\"relu\")\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "sgd_stats = train(model_ex4_sgd, train_dataset_cnn, test_dataset_cnn, optmzr=\"sgd\")\n",
    "adam_stats = train(model_ex4_adam, train_dataset_cnn, test_dataset_cnn, optmzr=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex4:\n",
    "# learning curve plot of adam vs sgd here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: residual connection\n",
    "\n",
    "Does this improve performance?\n",
    "\n",
    "Use the same structure as before but replace each conv+act pair with a block of two similar parts with a residual connection over each such block.\n",
    "\n",
    "So x -> conv1 -> relu = f(x) ==> x -> conv1 -> relu -> conv1 -> relu = g(x) + x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, channels, activator):\n",
    "        super().__init__()\n",
    "        if activator.lower() == \"relu\":\n",
    "            self.activator = torch.nn.ReLU()\n",
    "        if activator.lower() == \"tanh\":\n",
    "            self.activator = torch.nn.Tanh()\n",
    "\n",
    "        self.conv = torch.nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x  # Save input for residual connection\n",
    "        out = self.conv(x)\n",
    "        out = self.activator(out)\n",
    "        out = self.conv(out)\n",
    "        out = self.activator(out)\n",
    "        out += identity  # Add residual connection\n",
    "        return out\n",
    "\n",
    "class ResConvNetwork(torch.nn.Module):\n",
    "    def __init__(self, activator: str):\n",
    "        super().__init__()\n",
    "\n",
    "        # Reused layers\n",
    "        if activator.lower() == \"relu\":\n",
    "            self.activator = torch.nn.ReLU()\n",
    "        if activator.lower() == \"tanh\":\n",
    "            self.activator = torch.nn.Tanh()\n",
    "        \n",
    "        self.initial_conv = torch.nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
    "        self.resblock1 = ResidualBlock(8, \"relu\") # blir det inte lite konstigt med dimensionerna här?\n",
    "        self.conv2 = torch.nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.resblock2 = ResidualBlock(16, \"relu\")\n",
    "        self.conv3 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.resblock3 = ResidualBlock(32, \"relu\")\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fullyconnected = torch.nn.Linear(32*7*7, 10)\n",
    "\n",
    "        self.components = torch.nn.ModuleList([self.initial_conv,\n",
    "                                               self.resblock1,\n",
    "                                               self.conv2,\n",
    "                                               self.resblock2,\n",
    "                                               self.conv3,\n",
    "                                               self.resblock3,\n",
    "                                               self.flatten,\n",
    "                                               self.fullyconnected])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for l in self.components:\n",
    "            x = l(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sympy' has no attribute 'printing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m resnet_model \u001b[38;5;241m=\u001b[39m ResConvNetwork(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m x_train, x_test, trc2, tra2, tec2, tac2, time, avgspeed \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset_cnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptmzr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, traindata, testdata, num_epochs, batch_size, optmzr)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optmzr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optmzr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     30\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;66;03m# Standard parameters in Adam\u001b[39;00m\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\optim\\sgd.py:63\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable, fused)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov \u001b[38;5;129;01mand\u001b[39;00m (momentum \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dampening \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_supports_amp_scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\optim\\optimizer.py:377\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    374\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_compile.py:27\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\convert_frame.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalStateGuard\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logging\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyScalarRestartAnalysis\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config, exc, logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging, trace_rules, variables\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\exc.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompileId\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\_dynamo\\utils.py:66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytree\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fx\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ordered_set\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedSet\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     75\u001b[0m     Application,\n\u001b[0;32m     76\u001b[0m     CeilToInt,\n\u001b[0;32m     77\u001b[0m     CleanDiv,\n\u001b[0;32m     78\u001b[0m     FloorDiv,\n\u001b[0;32m     79\u001b[0m     FloorToInt,\n\u001b[0;32m     80\u001b[0m     IsNonOverlappingAndDenseIndicator,\n\u001b[0;32m     81\u001b[0m     Mod,\n\u001b[0;32m     82\u001b[0m     PythonMod,\n\u001b[0;32m     83\u001b[0m )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m int_oo\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprinters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PythonPrinter\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\utils\\_sympy\\functions.py:185\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gcd  \u001b[38;5;66;03m# type: ignore[return-value]  # remove in py3.12\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# It would be nice to have assertions on whether or not inputs is_integer\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# However, with bugs like https://github.com/sympy/sympy/issues/26620 sympy\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# sometimes inconsistently reports floats an integers.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Right now, FloorDiv de facto changes behavior if arguments are negative or\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# not, this can potentially cause correctness issues.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mFloorDiv\u001b[39;00m(sympy\u001b[38;5;241m.\u001b[39mFunction):\n\u001b[0;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    We maintain this so that:\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    1. We can use divisibility guards to simplify FloorDiv(a, b) to a / b.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m    NB: This is Python-style floor division, round to -Inf\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     nargs: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m,)\n",
      "File \u001b[1;32md:\\skola\\universitet\\2024_2025\\p3\\deeplearning\\torch_fuck\\lib\\site-packages\\torch\\utils\\_sympy\\functions.py:206\u001b[0m, in \u001b[0;36mFloorDiv\u001b[1;34m()\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdivisor\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m sympy\u001b[38;5;241m.\u001b[39mBasic:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sympystr\u001b[39m(\u001b[38;5;28mself\u001b[39m, printer: \u001b[43msympy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprinting\u001b[49m\u001b[38;5;241m.\u001b[39mStrPrinter) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    207\u001b[0m     base \u001b[38;5;241m=\u001b[39m printer\u001b[38;5;241m.\u001b[39mparenthesize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase, PRECEDENCE[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtom\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    208\u001b[0m     divisor \u001b[38;5;241m=\u001b[39m printer\u001b[38;5;241m.\u001b[39mparenthesize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdivisor, PRECEDENCE[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtom\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sympy' has no attribute 'printing'"
     ]
    }
   ],
   "source": [
    "resnet_model = ResConvNetwork(\"tanh\")\n",
    "\n",
    "x_train, x_test, trc2, tra2, tec2, tac2, time, avgspeed = train(resnet_model, train_dataset_cnn, test_dataset_cnn, batch_size=64, optmzr=\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: CNN with three variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo\n",
    "\n",
    "1. Swapa SGD mot ADAM och se till att det funkar\n",
    "2. Implementera residual connection och kolla vad som sker\n",
    "3. CNN with three variations:\n",
    "    1. Regularization\n",
    "    2. Hur olika djup påverkar träningshastigheten?\n",
    "    3. Hur olika learning rates påverkar träningshastigheten?\n",
    "\n",
    "Misc: på något vis få ut x-axel från train() så att man kan plotta flera samtidigt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_fuck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
